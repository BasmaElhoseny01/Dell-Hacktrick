# -*- coding: utf-8 -*-
"""eagle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G2RyZwO9ikbUR2l3nmlO6zzSWF7sztOO
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Model
from keras.callbacks import ModelCheckpoint
from keras.models import Model, load_model, Sequential
from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D
from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape
from keras.optimizers import Adam

# GRADED FUNCTION: model

def model(input_shape=(1997,101)):
    """
    Function creating the model's graph in Keras.

    Argument:
    input_shape -- shape of the model's input data (using Keras conventions)

    Returns:
    model -- Keras model instance
    """

    X_input = Input(shape = input_shape)
    # X_input = Input(shape = (1997,101))

    ### START CODE HERE ###

    # Step 1: CONV layer (≈4 lines)
    X = Conv1D(196, kernel_size=17, strides=4)(X_input)                                 # CONV1D
    X = BatchNormalization()(X)                                 # Batch normalization
    X = Activation('relu')(X)                                 # ReLu activation
    X = Dropout(0.8)(X)                                 # dropout (use 0.8)

    # Step 2: First GRU Layer (≈4 lines)
    X = GRU(units = 128, return_sequences = True)(X) # GRU (use 128 units and return the sequences)
    X = Dropout(0.8)(X)                                 # dropout (use 0.8)
    X = BatchNormalization()(X)                                 # Batch normalization

    # Step 3: Second GRU Layer (≈4 lines)
    X = GRU(units = 128, return_sequences = True)(X)   # GRU (use 128 units and return the sequences)
    X = Dropout(0.8)(X)                                 # dropout (use 0.8)
    X = BatchNormalization()(X)                                  # Batch normalization
    X = Dropout(0.8)(X)                                  # dropout (use 0.8)

    # Step 4: Time-distributed dense layer (≈1 line)
    X = TimeDistributed(Dense(1, activation = "sigmoid"))(X) # time distributed  (sigmoid)

    ### END CODE HERE ###

    model = Model(inputs = X_input, outputs = X)

    return model

model = model()
model.summary()

# opt = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)
opt = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=["accuracy"])

import numpy as np
from sklearn.model_selection import train_test_split
import sys

def Split_Data(real_dataset_path:str,fake_dataset_path:str,train_val_test_ratio:tuple=(0,0,0)):
  #TODO fix ratio of split
  # Load the spectrogram data from the .npz file
  real_data = np.load(real_dataset_path)
  real_data_spectrogram =real_data['x'][:,:real_data['x'].shape[1]-1,:]
  real_data_labels =real_data['y']
  # x=spectrogram_data_real['x'] #(750, 1998, 101)
  # y=spectrogram_data_real['y'] #(750, 496)

  # Load the spectrogram data from the .npz file
  fake_data = np.load(fake_dataset_path)
  # remove last row in fake data
  fake_data_spectrogram =fake_data['x'][:,:fake_data['x'].shape[1]-1,:]
  fake_data_labels =fake_data['y']

  # print(real_data_spectrogram.shape)
  # print(real_data_labels.shape)
  # print(fake_data_spectrogram.shape)
  # print(fake_data_labels.shape)
  # sys.exit()

  # Combine positive and negative words arrays
  # spectrogram_data =torch.cat((real_data_spectrogram, fake_data_spectrogram), dim=0)
  spectrogram_data =np.concatenate((real_data_spectrogram, fake_data_spectrogram), axis=0)

  # Combine positive and negative labels arrays
  labels = np.concatenate((real_data_labels, fake_data_labels), axis=0)

  # Generate permutation indices
  # indices = torch.randperm(len(labels))
  indices = np.random.permutation(len(labels))

  # Shuffle the dataset using permutation indices
  spectrogram_data = spectrogram_data[indices]
  labels = labels[indices]



  # Split to Train,test,Validate
  # Split X and Y into training, validation, and testing sets
  X_train, X_test, Y_train, Y_test = train_test_split(spectrogram_data, labels, test_size=0.2, random_state=42)
  # X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)
  X_val=None
  Y_val=None

  return X_train,Y_train,X_val,Y_val,X_test,Y_test

real_dataset_path='/content/drive/MyDrive/Footprints Datasets/real.npz'
fake_dataset_path='/content/drive/MyDrive/Footprints Datasets/fake.npz'
X_train,Y_train,_,_,X_val,Y_val=Split_Data(real_dataset_path=real_dataset_path,fake_dataset_path=fake_dataset_path)

print(np.shape(X_train))
print(np.shape(Y_train))
print(np.shape(X_val))
print(np.shape(Y_val))

model.fit(X_train,Y_train, batch_size = 5, epochs=1)

loss, acc = model.evaluate(X_val, Y_val)