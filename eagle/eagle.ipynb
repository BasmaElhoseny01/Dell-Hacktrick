{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "4UDbCkwxPbFx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEbsSR6rs_p5",
        "outputId": "6d2f6b8e-2809-49f8-a7d1-58c5128839fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data import *\n",
        "real_dataset_path='/content/drive/MyDrive/Footprints Datasets/real.npz'\n",
        "fake_dataset_path='/content/drive/MyDrive/Footprints Datasets/fake.npz'\n",
        "X_train,Y_train,_,_,X_val,Y_val=Split_Data(real_dataset_path=real_dataset_path,fake_dataset_path=fake_dataset_path)\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(Y_train))\n",
        "print(np.shape(X_val))\n",
        "print(np.shape(Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL5sZzXLQlsz",
        "outputId": "166054f1-f675-4cf2-cb8b-ac7546085b96"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 1997, 101)\n",
            "(1200, 496)\n",
            "(300, 1997, 101)\n",
            "(300, 496)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Minimum value\",  np.min(X_train))\n",
        "print(\"Maximum value\", np.max(X_train))\n",
        "X_train=preprocessing(X_train)\n",
        "print(\"Minimum value\",  np.min(X_train))\n",
        "print(\"Maximum value\", np.max(X_train))\n",
        "\n",
        "\n",
        "print(\"Minimum value\",  np.min(X_val))\n",
        "print(\"Maximum value\", np.max(X_val))\n",
        "X_val=preprocessing(X_val)\n",
        "print(\"Minimum value\",  np.min(X_val))\n",
        "print(\"Maximum value\", np.max(X_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3KYMkWoQoCv",
        "outputId": "bf33f635-310c-4fd2-9f6c-1c41f0bd3649"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value 0.0\n",
            "Maximum value inf\n",
            "Minimum value 0.0\n",
            "Maximum value 65500.0\n",
            "Minimum value 0.0\n",
            "Maximum value inf\n",
            "Minimum value 0.0\n",
            "Maximum value 65500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzgXCQvybz_5",
        "outputId": "4ad70a3a-5a0f-4266-db7c-4e3aee49689c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.2, 1: 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "sMz8hYwJrBrQ"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
        "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomUniform\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7oXKS8SkrBrS"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(input_shape=(1997,101)):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras.\n",
        "\n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "\n",
        "    X_input = Input(shape = input_shape)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Step 1: CONV layer (≈4 lines)\n",
        "    X = Conv1D(196, kernel_size=17, strides=4)(X_input)                                 # CONV1D\n",
        "    X = BatchNormalization()(X)                                 # Batch normalization\n",
        "    X = Activation('relu')(X)                                 # ReLu activation\n",
        "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
        "\n",
        "    #   # Print output shape after Conv1D\n",
        "    # print(\"Output shape after Conv1D:\", X.shape)\n",
        "\n",
        "    # Step 2: First GRU Layer (≈4 lines)\n",
        "    X = GRU(units = 128, return_sequences = True)(X) # GRU (use 128 units and return the sequences)\n",
        "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
        "    X = BatchNormalization()(X)                                 # Batch normalization\n",
        "\n",
        "    # # Step 3: Second GRU Layer (≈4 lines)\n",
        "    X = GRU(units = 128, return_sequences = True)(X)   # GRU (use 128 units and return the sequences)\n",
        "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
        "    X = BatchNormalization()(X)                                  # Batch normalization\n",
        "    X = Dropout(0.8)(X)                                  # dropout (use 0.8)\n",
        "\n",
        "    # # Step 4: Time-distributed dense layer (≈1 line)\n",
        "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZThW65irBrT",
        "outputId": "1a9dcb4c-d4e3-4cb3-9a1b-c2a253b705ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 1997, 101)]       0         \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 496, 196)          336728    \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 496, 196)          784       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 496, 196)          0         \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 496, 196)          0         \n",
            "                                                                 \n",
            " gru_12 (GRU)                (None, 496, 128)          125184    \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 496, 128)          0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 496, 128)          512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " gru_13 (GRU)                (None, 496, 128)          99072     \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 496, 128)          0         \n",
            "                                                                 \n",
            " batch_normalization_20 (Ba  (None, 496, 128)          512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 496, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDi  (None, 496, 1)            129       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 562921 (2.15 MB)\n",
            "Trainable params: 562017 (2.14 MB)\n",
            "Non-trainable params: 904 (3.53 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_eagle = model()\n",
        "model_eagle.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = np.count_nonzero(Y_train == 0)\n",
        "ones = np.count_nonzero(Y_train == 1)\n",
        "print(f\"Real Data: 0s: {zeros}, 1s:{ones}\")\n",
        "print(zeros/ones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh6iiVK6u2Nf",
        "outputId": "5ac5cf9a-ac82-4741-c7eb-c0002835476c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real Data: 0s: 535701, 1s:59499\n",
            "9.003529471083548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {0: 1.0, 1: 10.0}  # Example class weights\n",
        "@keras.saving.register_keras_serializable()\n",
        "# Define weighted binary cross-entropy loss function\n",
        "def weighted_binary_crossentropy(y_true, y_pred):\n",
        "    # Clip predicted values to prevent log(0) and log(1) cases\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Compute weighted binary cross-entropy\n",
        "    weights = tf.constant([class_weights[i] for i in range(len(class_weights))])\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    return tf.reduce_mean(bce(y_true, y_pred) * weights)"
      ],
      "metadata": {
        "id": "diixg2b4dzQq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "p8YDHcxarBrT"
      },
      "outputs": [],
      "source": [
        "# opt = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "model_eagle.compile(loss=weighted_binary_crossentropy, optimizer=opt, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 5, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE5MJPMxg_yt",
        "outputId": "a420263d-eadd-40cf-817c-46507fd60430"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240/240 [==============================] - 14s 42ms/step - loss: 3.4101 - accuracy: 0.7822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db2d7853760>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 16, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnE4l9kzs0eH",
        "outputId": "4f8410d5-b4bd-4bac-8309-8205d614924d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "75/75 [==============================] - 6s 48ms/step - loss: 1.9000 - accuracy: 0.8957\n",
            "Epoch 2/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.8285 - accuracy: 0.8981\n",
            "Epoch 3/30\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 1.8075 - accuracy: 0.8989\n",
            "Epoch 4/30\n",
            "75/75 [==============================] - 3s 47ms/step - loss: 1.7998 - accuracy: 0.8992\n",
            "Epoch 5/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7926 - accuracy: 0.8994\n",
            "Epoch 6/30\n",
            "75/75 [==============================] - 4s 52ms/step - loss: 1.7889 - accuracy: 0.8996\n",
            "Epoch 7/30\n",
            "75/75 [==============================] - 4s 49ms/step - loss: 1.7903 - accuracy: 0.8996\n",
            "Epoch 8/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7837 - accuracy: 0.8997\n",
            "Epoch 9/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7799 - accuracy: 0.8998\n",
            "Epoch 10/30\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 1.7806 - accuracy: 0.8998\n",
            "Epoch 11/30\n",
            "75/75 [==============================] - 3s 47ms/step - loss: 1.7789 - accuracy: 0.8998\n",
            "Epoch 12/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7744 - accuracy: 0.8998\n",
            "Epoch 13/30\n",
            "75/75 [==============================] - 4s 51ms/step - loss: 1.7757 - accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "75/75 [==============================] - 4s 50ms/step - loss: 1.7759 - accuracy: 0.8983\n",
            "Epoch 15/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7761 - accuracy: 0.8999\n",
            "Epoch 16/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7718 - accuracy: 0.8999\n",
            "Epoch 17/30\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 1.8162 - accuracy: 0.8970\n",
            "Epoch 18/30\n",
            "75/75 [==============================] - 3s 46ms/step - loss: 1.7787 - accuracy: 0.8998\n",
            "Epoch 19/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 1.7707 - accuracy: 0.8999\n",
            "Epoch 20/30\n",
            "75/75 [==============================] - 4s 50ms/step - loss: 1.7530 - accuracy: 0.8989\n",
            "Epoch 21/30\n",
            "75/75 [==============================] - 4s 53ms/step - loss: 1.5675 - accuracy: 0.8973\n",
            "Epoch 22/30\n",
            "75/75 [==============================] - 3s 46ms/step - loss: 1.1444 - accuracy: 0.9067\n",
            "Epoch 23/30\n",
            "75/75 [==============================] - 3s 46ms/step - loss: 1.0380 - accuracy: 0.9135\n",
            "Epoch 24/30\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.9650 - accuracy: 0.9228\n",
            "Epoch 25/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.9222 - accuracy: 0.9284\n",
            "Epoch 26/30\n",
            "75/75 [==============================] - 3s 46ms/step - loss: 0.8149 - accuracy: 0.9331\n",
            "Epoch 27/30\n",
            "75/75 [==============================] - 4s 48ms/step - loss: 0.8458 - accuracy: 0.9348\n",
            "Epoch 28/30\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.8094 - accuracy: 0.9349\n",
            "Epoch 29/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.7087 - accuracy: 0.9436\n",
            "Epoch 30/30\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.6992 - accuracy: 0.9437\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db2d78a9b40>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 16, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy731CUJim-k",
        "outputId": "4a9f5fff-e5fe-4b36-ca05-d5783d94865f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 4s 48ms/step - loss: 0.6857 - accuracy: 0.9455\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.6733 - accuracy: 0.9493\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 3s 46ms/step - loss: 0.6974 - accuracy: 0.9449\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.6392 - accuracy: 0.9526\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 4s 48ms/step - loss: 0.5661 - accuracy: 0.9565\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 4s 53ms/step - loss: 0.5614 - accuracy: 0.9584\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.5319 - accuracy: 0.9606\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.5600 - accuracy: 0.9585\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 4s 52ms/step - loss: 0.4989 - accuracy: 0.9629\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 4s 49ms/step - loss: 0.5514 - accuracy: 0.9593\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db27c3433d0>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 16, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJfCfjBKeIYV",
        "outputId": "104a84a5-555f-4e78-f608-745e82b3c5e3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 4s 48ms/step - loss: 0.5015 - accuracy: 0.9633\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.4723 - accuracy: 0.9657\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.4319 - accuracy: 0.9692\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.4542 - accuracy: 0.9682\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 4s 51ms/step - loss: 0.4635 - accuracy: 0.9674\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 4s 51ms/step - loss: 0.4174 - accuracy: 0.9704\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 4s 47ms/step - loss: 0.3931 - accuracy: 0.9725\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 4s 48ms/step - loss: 0.4317 - accuracy: 0.9701\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 0.4292 - accuracy: 0.9700\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 3s 47ms/step - loss: 0.4760 - accuracy: 0.9676\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db287baa350>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 5, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bctOTr7Qr0-X",
        "outputId": "21d6f082-adfc-40cb-910e-c1248915876c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240/240 [==============================] - 142s 573ms/step - loss: 0.9543 - accuracy: 0.5938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b601830c9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 5, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSM_J1EORqg-",
        "outputId": "3e7f6a26-56da-47b7-da6f-a5345ceba85a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240/240 [==============================] - 141s 589ms/step - loss: 0.7612 - accuracy: 0.7689\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b6018103610>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.fit(X_train,Y_train, batch_size = 64, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snzKfR3rV3H-",
        "outputId": "c1d19e4c-3968-4016-8a53-c8d4fb87a374"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 71s 3s/step - loss: 0.6902 - accuracy: 0.8644\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b60182f9480>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eagle.save('/content/model_1.keras')"
      ],
      "metadata": {
        "id": "dggXD-A1xE9N"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model_eagle.evaluate(X_val, Y_val)\n",
        "print(loss)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSK_CiMevtfU",
        "outputId": "9a883616-fd40-4dda-86bb-9822b4112451"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 43ms/step - loss: 0.2799 - accuracy: 0.9796\n",
            "0.27992844581604004\n",
            "0.9796304106712341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_eagle.predict(X_val)\n",
        "print(np.shape(predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oql8qK1dRiNz",
        "outputId": "bd93badc-8907-4446-be23-47f061af759b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 37ms/step\n",
            "(300, 496, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[0,:].T)\n",
        "print(np.max(predictions[0,:]))\n",
        "\n",
        "# Apply threshold of 0.5 to convert probabilities to binary labels\n",
        "binary_labels = (predictions[0,:] >= 0.5).astype(int)\n",
        "print(binary_labels.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAJ3F8eUSyKs",
        "outputId": "6364c8a0-7866-4e33-90ad-3e41d4d69808"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.79759991e-02 4.11665998e-02 2.04048753e-02 1.16588390e-02\n",
            "  7.34589295e-03 4.88874502e-03 3.31032928e-03 2.21129600e-03\n",
            "  1.42732123e-03 8.89947347e-04 5.54073660e-04 3.66203400e-04\n",
            "  2.70630320e-04 2.25537369e-04 2.06012715e-04 1.98628200e-04\n",
            "  1.96395456e-04 1.95884350e-04 1.95656234e-04 1.95303204e-04\n",
            "  1.94825945e-04 1.94332621e-04 1.93928747e-04 1.93674670e-04\n",
            "  1.93572749e-04 1.93603395e-04 1.93718050e-04 1.93893778e-04\n",
            "  1.94093940e-04 1.94315580e-04 1.94533772e-04 1.94752967e-04\n",
            "  1.94976295e-04 1.95201923e-04 1.95411252e-04 1.95605491e-04\n",
            "  1.95781278e-04 1.95945046e-04 1.96111403e-04 1.96284993e-04\n",
            "  1.96437410e-04 1.96574154e-04 1.96692665e-04 1.96844077e-04\n",
            "  1.96966866e-04 1.97019443e-04 1.97064903e-04 1.97155707e-04\n",
            "  1.97268528e-04 1.97407193e-04 1.97504734e-04 1.97561414e-04\n",
            "  1.97589296e-04 1.97652218e-04 1.97780057e-04 1.97852278e-04\n",
            "  1.97885107e-04 1.97883041e-04 1.97938323e-04 1.98003821e-04\n",
            "  1.98072754e-04 1.98144335e-04 1.98201786e-04 1.98236376e-04\n",
            "  1.98275506e-04 1.98313312e-04 1.98358524e-04 1.98400870e-04\n",
            "  1.98454029e-04 1.98497364e-04 1.98514419e-04 1.98520676e-04\n",
            "  1.98557187e-04 1.98613052e-04 1.98639187e-04 1.98666472e-04\n",
            "  1.98675567e-04 1.98671012e-04 1.98671580e-04 1.98696769e-04\n",
            "  1.98713256e-04 1.98739595e-04 1.98784721e-04 1.98816924e-04\n",
            "  1.98828289e-04 1.98843467e-04 1.98862428e-04 1.98897513e-04\n",
            "  1.98992537e-04 1.99030095e-04 1.98980968e-04 1.98909460e-04\n",
            "  1.98925205e-04 1.98954222e-04 1.98970709e-04 1.99013215e-04\n",
            "  1.99034665e-04 1.99057438e-04 1.99049653e-04 1.99056114e-04\n",
            "  1.99083457e-04 1.99105270e-04 1.99113638e-04 1.99119910e-04\n",
            "  1.99114394e-04 1.99118935e-04 1.99120666e-04 1.99115719e-04\n",
            "  1.99118382e-04 1.99116650e-04 1.99120666e-04 1.99134505e-04\n",
            "  1.99170026e-04 1.99170216e-04 1.99179325e-04 1.99166607e-04\n",
            "  1.99158632e-04 1.99230970e-04 1.99272399e-04 1.99256829e-04\n",
            "  1.99244663e-04 1.99197544e-04 1.99180460e-04 1.99263683e-04\n",
            "  1.99486283e-04 1.99474467e-04 1.99249596e-04 1.99091417e-04\n",
            "  1.99109258e-04 1.99127302e-04 1.99191112e-04 1.99267277e-04\n",
            "  1.99286282e-04 1.99264236e-04 1.99247705e-04 1.99249596e-04\n",
            "  1.99258910e-04 1.99249043e-04 1.99242204e-04 1.99256450e-04\n",
            "  1.99316491e-04 1.99342903e-04 1.99424088e-04 1.99556278e-04\n",
            "  1.99625181e-04 1.99421032e-04 1.99156915e-04 1.99084971e-04\n",
            "  1.99169634e-04 1.99211048e-04 1.99322574e-04 1.99510832e-04\n",
            "  1.99667818e-04 1.99746079e-04 1.99770075e-04 1.99725910e-04\n",
            "  1.99637361e-04 1.99458300e-04 1.99225105e-04 1.98992537e-04\n",
            "  1.98858063e-04 1.98792273e-04 1.98773530e-04 1.98775786e-04\n",
            "  1.98759313e-04 1.98758338e-04 1.98789814e-04 1.98856549e-04\n",
            "  1.98907190e-04 1.98919137e-04 1.98950613e-04 1.99002592e-04\n",
            "  1.99072063e-04 1.99103379e-04 1.99143236e-04 1.99191112e-04\n",
            "  1.99198519e-04 1.99213900e-04 1.99346716e-04 1.99367249e-04\n",
            "  1.99253234e-04 1.99091999e-04 1.99086091e-04 1.99189395e-04\n",
            "  1.99286660e-04 1.99334740e-04 1.99324277e-04 1.99309463e-04\n",
            "  1.99299204e-04 1.99291972e-04 1.99296162e-04 1.99294067e-04\n",
            "  1.99279253e-04 1.99259492e-04 1.99230417e-04 1.99206494e-04\n",
            "  1.99202128e-04 1.99204587e-04 1.99211616e-04 1.99203831e-04\n",
            "  1.99201546e-04 1.99209899e-04 1.99214264e-04 1.99216927e-04\n",
            "  1.99240880e-04 1.99272225e-04 1.99277914e-04 1.99277914e-04\n",
            "  1.99290458e-04 1.99298258e-04 1.99311558e-04 1.99306785e-04\n",
            "  1.99315167e-04 1.99351460e-04 1.99350121e-04 1.99307557e-04\n",
            "  1.99317627e-04 1.99319722e-04 1.99345566e-04 1.99364586e-04\n",
            "  1.99361530e-04 1.99294824e-04 1.99238202e-04 1.99286282e-04\n",
            "  1.99346716e-04 1.99349932e-04 1.99305272e-04 1.99309288e-04\n",
            "  1.99342903e-04 1.99342714e-04 1.99301110e-04 1.99291972e-04\n",
            "  1.99314221e-04 1.99319329e-04 1.99277347e-04 1.99273360e-04\n",
            "  1.99277158e-04 1.99294824e-04 1.99282294e-04 1.99280199e-04\n",
            "  1.99294445e-04 1.99302638e-04 1.99287417e-04 1.99272792e-04\n",
            "  1.99270507e-04 1.99282673e-04 1.99286078e-04 1.99279253e-04\n",
            "  1.99282847e-04 1.99283604e-04 1.99282673e-04 1.99303002e-04\n",
            "  1.99307193e-04 1.99281145e-04 1.99251313e-04 1.99246002e-04\n",
            "  1.99256072e-04 1.99287999e-04 1.99300543e-04 1.99324480e-04\n",
            "  1.99299015e-04 1.99240691e-04 1.99213318e-04 1.99242393e-04\n",
            "  1.99261194e-04 1.99264992e-04 1.99272981e-04 1.99282294e-04\n",
            "  1.99279253e-04 1.99269547e-04 1.99267088e-04 1.99292161e-04\n",
            "  1.99297865e-04 1.99311165e-04 1.99311558e-04 1.99293136e-04\n",
            "  1.99262344e-04 1.99262518e-04 1.99282294e-04 1.99299204e-04\n",
            "  1.99314221e-04 1.99319329e-04 1.99300353e-04 1.99292932e-04\n",
            "  1.99304908e-04 1.99301663e-04 1.99286078e-04 1.99287606e-04\n",
            "  1.99288756e-04 1.99285700e-04 1.99288756e-04 1.99293689e-04\n",
            "  1.99298258e-04 1.99301474e-04 1.99304908e-04 1.99303933e-04\n",
            "  1.99302056e-04 1.99297676e-04 1.99291040e-04 1.99288945e-04\n",
            "  1.99284390e-04 1.99275819e-04 1.99266331e-04 1.99263275e-04\n",
            "  1.99260641e-04 1.99273185e-04 1.99273738e-04 1.99276954e-04\n",
            "  1.99277347e-04 1.99275441e-04 1.99274305e-04 1.99289119e-04\n",
            "  1.99308881e-04 1.99304151e-04 1.99281523e-04 1.99272225e-04\n",
            "  1.99287795e-04 1.99308881e-04 1.99307950e-04 1.99283808e-04\n",
            "  1.99282673e-04 1.99290458e-04 1.99293689e-04 1.99277914e-04\n",
            "  1.99297298e-04 1.99368762e-04 1.99375048e-04 1.99303176e-04\n",
            "  1.99242975e-04 1.99116475e-04 1.98955924e-04 1.98410722e-04\n",
            "  1.96752691e-04 1.93730986e-04 1.84365519e-04 1.62474214e-04\n",
            "  1.11603593e-04 5.35894578e-05 1.58194343e-05 4.32108664e-06\n",
            "  2.08384836e-06 1.83735460e-06 5.03083982e-07 3.61514623e-07\n",
            "  3.01697213e-07 2.72957664e-07 2.42545866e-07 2.24182443e-07\n",
            "  2.11436500e-07 2.02810114e-07 2.02158390e-07 2.06913555e-07\n",
            "  2.03742189e-07 2.06123985e-07 2.04174597e-07 2.10686323e-07\n",
            "  2.05232681e-07 2.14121926e-07 2.08906826e-07 2.18839403e-07\n",
            "  2.25963163e-07 2.12377032e-07 2.11504641e-07 2.14670933e-07\n",
            "  2.15789797e-07 2.15666958e-07 2.44853226e-07 4.02783996e-07\n",
            "  2.97464499e-06 4.02616679e-05 3.05264810e-04 9.45518666e-04\n",
            "  1.35979953e-03 1.17007631e-03 7.60340539e-04 4.30657645e-04\n",
            "  2.38271445e-04 1.45469632e-04 1.09358734e-04 1.04470331e-04\n",
            "  1.18953692e-04 1.47507832e-04 1.85731362e-04 2.27611017e-04\n",
            "  2.65922310e-04 2.94350320e-04 3.09590221e-04 3.11990560e-04\n",
            "  3.04645830e-04 2.91583448e-04 2.76342005e-04 2.61454901e-04\n",
            "  2.48338503e-04 2.37473781e-04 2.28770688e-04 2.21958137e-04\n",
            "  2.16703091e-04 2.12686893e-04 2.09599297e-04 2.07200967e-04\n",
            "  2.05380769e-04 2.03992400e-04 2.02918847e-04 2.02113268e-04\n",
            "  2.01526927e-04 2.01076255e-04 2.00739480e-04 2.00490816e-04\n",
            "  2.00300114e-04 2.00159993e-04 2.00063834e-04 1.99998205e-04\n",
            "  1.99933944e-04 1.99886490e-04 1.99868955e-04 1.99872782e-04\n",
            "  1.99846283e-04 1.99835806e-04 1.99814269e-04 1.99869537e-04\n",
            "  1.99923859e-04 1.99866307e-04 1.99782837e-04 1.99780552e-04\n",
            "  1.99832197e-04 1.99768183e-04 1.99660382e-04 1.99588831e-04\n",
            "  1.99635455e-04 1.99723974e-04 1.99782444e-04 1.99739618e-04\n",
            "  1.99627262e-04 1.99550952e-04 1.99533068e-04 1.99551345e-04\n",
            "  1.99583315e-04 1.99577786e-04 1.99532311e-04 1.99487040e-04\n",
            "  1.99513859e-04 1.99532311e-04 1.99517483e-04 1.99496542e-04\n",
            "  1.99491216e-04 1.99475253e-04 1.99462302e-04 1.99454124e-04\n",
            "  1.99437214e-04 1.99427697e-04 1.99443093e-04 1.99460032e-04\n",
            "  1.99468195e-04 1.99468384e-04 1.99450325e-04 1.99431888e-04\n",
            "  1.99431699e-04 1.99436065e-04 1.99418573e-04 1.99398433e-04\n",
            "  1.99409071e-04 1.99435119e-04 1.99445582e-04 1.99450325e-04\n",
            "  1.99443661e-04 1.99428643e-04 1.99420087e-04 1.99429604e-04\n",
            "  1.99449176e-04 1.99446906e-04 1.99419155e-04 1.99381117e-04\n",
            "  1.99354487e-04 1.99353730e-04 1.99359638e-04 1.99366288e-04\n",
            "  1.99377115e-04 1.99366870e-04 1.99365531e-04 1.99346512e-04\n",
            "  1.99333794e-04 1.99322763e-04 1.99329603e-04 1.99343471e-04\n",
            "  1.99350339e-04 1.99344242e-04 1.99335496e-04 1.99348608e-04\n",
            "  1.99345755e-04 1.99340429e-04 1.99324655e-04 1.99326183e-04\n",
            "  1.99326576e-04 1.99352799e-04 1.99336078e-04 1.99307004e-04]]\n",
            "0.097976\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_val[0])\n",
        "print(np.max(Y_val[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mMs4IQGU2Uk",
        "outputId": "9231a248-301f-46fc-e358-5758ea13be17"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[1].T)\n",
        "print(np.max(predictions[1]))\n",
        "\n",
        "# Apply threshold of 0.5 to convert probabilities to binary labels\n",
        "binary_labels = (predictions[1,:] >= 0.5).astype(int)\n",
        "print(binary_labels.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hine8IEpSuwt",
        "outputId": "58a3b21e-ef95-4939-882d-ccdefe41d6f0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.7975999e-02 4.1166607e-02 2.0404875e-02 1.1658839e-02 7.3458930e-03\n",
            "  4.8887450e-03 3.3103267e-03 2.2112823e-03 1.4272743e-03 8.8991004e-04\n",
            "  5.5406045e-04 3.6620360e-04 2.7062825e-04 2.2553437e-04 2.0600896e-04\n",
            "  1.9862063e-04 1.9638235e-04 1.9586658e-04 1.9564168e-04 1.9528181e-04\n",
            "  1.9479661e-04 1.9430985e-04 1.9391951e-04 1.9367263e-04 1.9356649e-04\n",
            "  1.9357498e-04 1.9367042e-04 1.9383040e-04 1.9402528e-04 1.9424113e-04\n",
            "  1.9446108e-04 1.9467017e-04 1.9486812e-04 1.9505345e-04 1.9522074e-04\n",
            "  1.9540901e-04 1.9561202e-04 1.9581561e-04 1.9601568e-04 1.9620343e-04\n",
            "  1.9636961e-04 1.9653051e-04 1.9667747e-04 1.9681497e-04 1.9692742e-04\n",
            "  1.9700835e-04 1.9708145e-04 1.9714273e-04 1.9722413e-04 1.9732081e-04\n",
            "  1.9742431e-04 1.9752544e-04 1.9761473e-04 1.9767598e-04 1.9772745e-04\n",
            "  1.9777892e-04 1.9784021e-04 1.9790491e-04 1.9797380e-04 1.9803537e-04\n",
            "  1.9808502e-04 1.9812393e-04 1.9816929e-04 1.9823202e-04 1.9827209e-04\n",
            "  1.9831615e-04 1.9836740e-04 1.9841772e-04 1.9846503e-04 1.9851346e-04\n",
            "  1.9856117e-04 1.9859146e-04 1.9860112e-04 1.9861154e-04 1.9861797e-04\n",
            "  1.9862782e-04 1.9864090e-04 1.9865263e-04 1.9868939e-04 1.9872464e-04\n",
            "  1.9875211e-04 1.9878017e-04 1.9880044e-04 1.9881636e-04 1.9885031e-04\n",
            "  1.9888215e-04 1.9889939e-04 1.9889978e-04 1.9887589e-04 1.9885464e-04\n",
            "  1.9884044e-04 1.9885199e-04 1.9888025e-04 1.9891343e-04 1.9894265e-04\n",
            "  1.9897052e-04 1.9899823e-04 1.9902120e-04 1.9903675e-04 1.9904321e-04\n",
            "  1.9904246e-04 1.9902480e-04 1.9902480e-04 1.9903656e-04 1.9904396e-04\n",
            "  1.9906105e-04 1.9906390e-04 1.9911439e-04 1.9914817e-04 1.9920859e-04\n",
            "  1.9925796e-04 1.9927202e-04 1.9927374e-04 1.9928456e-04 1.9926784e-04\n",
            "  1.9925930e-04 1.9925759e-04 1.9923155e-04 1.9920610e-04 1.9918104e-04\n",
            "  1.9916623e-04 1.9917876e-04 1.9915595e-04 1.9911857e-04 1.9910299e-04\n",
            "  1.9907624e-04 1.9905651e-04 1.9907109e-04 1.9911288e-04 1.9912900e-04\n",
            "  1.9911723e-04 1.9909332e-04 1.9908079e-04 1.9909255e-04 1.9914458e-04\n",
            "  1.9922186e-04 1.9926101e-04 1.9929331e-04 1.9923042e-04 1.9920213e-04\n",
            "  1.9921274e-04 1.9923555e-04 1.9929314e-04 1.9934177e-04 1.9937105e-04\n",
            "  1.9937768e-04 1.9937505e-04 1.9939539e-04 1.9937940e-04 1.9935773e-04\n",
            "  1.9933892e-04 1.9932029e-04 1.9930812e-04 1.9930737e-04 1.9932486e-04\n",
            "  1.9934386e-04 1.9935146e-04 1.9935261e-04 1.9934824e-04 1.9934861e-04\n",
            "  1.9936022e-04 1.9938320e-04 1.9939974e-04 1.9939196e-04 1.9936268e-04\n",
            "  1.9933740e-04 1.9932239e-04 1.9932087e-04 1.9930776e-04 1.9930281e-04\n",
            "  1.9930719e-04 1.9933208e-04 1.9936419e-04 1.9937826e-04 1.9938225e-04\n",
            "  1.9937809e-04 1.9933379e-04 1.9932180e-04 1.9931232e-04 1.9930795e-04\n",
            "  1.9934063e-04 1.9935222e-04 1.9935489e-04 1.9936591e-04 1.9933493e-04\n",
            "  1.9929750e-04 1.9927145e-04 1.9926444e-04 1.9925265e-04 1.9926784e-04\n",
            "  1.9928077e-04 1.9929273e-04 1.9930015e-04 1.9930415e-04 1.9929314e-04\n",
            "  1.9929350e-04 1.9929104e-04 1.9929140e-04 1.9929159e-04 1.9928305e-04\n",
            "  1.9927886e-04 1.9927850e-04 1.9928532e-04 1.9927791e-04 1.9926709e-04\n",
            "  1.9923632e-04 1.9921809e-04 1.9919357e-04 1.9919567e-04 1.9919357e-04\n",
            "  1.9920687e-04 1.9919813e-04 1.9918274e-04 1.9920363e-04 1.9924466e-04\n",
            "  1.9928685e-04 1.9933170e-04 1.9936192e-04 1.9935146e-04 1.9935163e-04\n",
            "  1.9934215e-04 1.9932599e-04 1.9932733e-04 1.9932845e-04 1.9929938e-04\n",
            "  1.9926119e-04 1.9920859e-04 1.9915595e-04 1.9911893e-04 1.9911589e-04\n",
            "  1.9915331e-04 1.9922301e-04 1.9930206e-04 1.9938342e-04 1.9943151e-04\n",
            "  1.9946936e-04 1.9968591e-04 2.0405912e-04 1.8750185e-04 1.1028716e-05\n",
            "  2.2904298e-06 1.2484661e-06 8.9407445e-07 7.1420516e-07 5.7368527e-07\n",
            "  4.6816135e-07 4.0798454e-07 3.3926841e-07 2.8535774e-07 2.3779926e-07\n",
            "  2.0657976e-07 1.8915669e-07 1.7665087e-07 1.7073066e-07 1.6864280e-07\n",
            "  1.6355665e-07 1.5936202e-07 1.5763749e-07 1.5363831e-07 1.4784415e-07\n",
            "  1.4229779e-07 1.3877775e-07 1.4002622e-07 1.4082869e-07 1.4715782e-07\n",
            "  1.6584829e-07 2.3520798e-07 5.0974774e-07 2.8737097e-06 2.1722532e-05\n",
            "  1.2959969e-04 5.3408521e-04 1.6817499e-03 4.3705245e-03 9.7913574e-03\n",
            "  1.9472834e-02 3.5074465e-02 5.8049321e-02 8.9264102e-02 1.2867670e-01\n",
            "  1.7523713e-01 2.2704504e-01 2.8174460e-01 3.3696502e-01 3.9070663e-01\n",
            "  4.4154817e-01 4.8868066e-01 5.3180426e-01 5.7100677e-01 6.0661203e-01\n",
            "  6.3907284e-01 6.6889262e-01 6.9654167e-01 7.2243822e-01 7.4689603e-01\n",
            "  7.7011603e-01 7.9219264e-01 8.1313711e-01 8.3290040e-01 8.5140419e-01\n",
            "  8.6856824e-01 8.8432735e-01 8.9864469e-01 9.1152233e-01 9.2299271e-01\n",
            "  9.3312114e-01 9.4199395e-01 9.4971496e-01 9.5639473e-01 9.6214604e-01\n",
            "  9.6707958e-01 9.7129929e-01 9.7490102e-01 9.7797090e-01 9.8058528e-01\n",
            "  9.8281121e-01 9.8470646e-01 9.8632091e-01 9.8769683e-01 9.8887014e-01\n",
            "  9.8987108e-01 9.9072510e-01 9.9145389e-01 9.9207509e-01 9.9260396e-01\n",
            "  9.9305290e-01 9.9343228e-01 9.9375087e-01 9.9401581e-01 9.9423295e-01\n",
            "  9.9440706e-01 9.9454170e-01 9.9464029e-01 9.9470478e-01 9.9473697e-01\n",
            "  9.9473828e-01 9.9470913e-01 9.9464983e-01 9.9455982e-01 9.9443829e-01\n",
            "  9.9428374e-01 9.9409455e-01 9.9386805e-01 9.9360162e-01 9.9329162e-01\n",
            "  9.9293411e-01 9.9252385e-01 9.9205512e-01 9.9152112e-01 9.9091434e-01\n",
            "  9.9022561e-01 9.8944491e-01 9.8856038e-01 9.8755860e-01 9.8642415e-01\n",
            "  9.8514026e-01 9.8368651e-01 9.8204023e-01 9.8017591e-01 9.7806346e-01\n",
            "  9.7567093e-01 9.7296101e-01 9.6989238e-01 9.6641958e-01 9.6249205e-01\n",
            "  9.5805359e-01 9.5304364e-01 9.4739622e-01 9.4103920e-01 9.3389434e-01\n",
            "  9.2588013e-01 9.1690999e-01 9.0689284e-01 8.9574349e-01 8.8337535e-01\n",
            "  8.6970848e-01 8.5466665e-01 8.3818704e-01 8.2021862e-01 8.0072600e-01\n",
            "  7.7970469e-01 7.5717282e-01 7.3318124e-01 7.0780969e-01 6.8116868e-01\n",
            "  6.5340000e-01 6.2467557e-01 5.9519374e-01 5.6517142e-01 5.3484321e-01\n",
            "  5.0444865e-01 4.7422320e-01 4.4439548e-01 4.1517720e-01 3.8675666e-01\n",
            "  3.5930181e-01 3.3295152e-01 3.0781189e-01 2.8395680e-01 2.6144210e-01\n",
            "  2.4028976e-01 2.2049604e-01 2.0204858e-01 1.8491559e-01 1.6904549e-01\n",
            "  1.5438935e-01 1.4089042e-01 1.2847684e-01 1.1707637e-01 1.0661843e-01\n",
            "  9.7030744e-02 8.8243507e-02 8.0194995e-02 7.2823174e-02 6.6070840e-02\n",
            "  5.9885073e-02 5.4214865e-02 4.9012505e-02 4.4235829e-02 3.9846171e-02\n",
            "  3.5806827e-02 3.2085467e-02 2.8651461e-02 2.5477229e-02 2.2538397e-02\n",
            "  1.9812893e-02 1.7281443e-02 1.4926886e-02 1.2734572e-02 1.0693203e-02\n",
            "  8.7951319e-03 7.0376662e-03 5.4255151e-03 3.9736000e-03 2.7129643e-03\n",
            "  1.6913562e-03 9.5767167e-04 5.1749020e-04 3.0207672e-04 2.1260315e-04\n",
            "  1.8067026e-04 1.7324885e-04 1.7543885e-04 1.8034257e-04 1.8490493e-04\n",
            "  1.8817923e-04 1.9019903e-04 1.9135539e-04 1.9201117e-04 1.9243619e-04\n",
            "  1.9280333e-04 1.9311592e-04 1.9343106e-04 1.9370957e-04 1.9399534e-04\n",
            "  1.9424390e-04 1.9449501e-04 1.9473086e-04 1.9496289e-04 1.9518536e-04\n",
            "  1.9537995e-04 1.9556652e-04 1.9576373e-04 1.9597681e-04 1.9618118e-04\n",
            "  1.9638102e-04 1.9652807e-04 1.9666192e-04 1.9676487e-04 1.9683767e-04\n",
            "  1.9690602e-04 1.9701268e-04 1.9706566e-04 1.9713595e-04 1.9720590e-04\n",
            "  1.9729034e-04 1.9738817e-04 1.9750228e-04 1.9761924e-04 1.9768579e-04\n",
            "  1.9774704e-04 1.9781965e-04 1.9789340e-04 1.9797402e-04 1.9803742e-04\n",
            "  1.9805990e-04 1.9806708e-04 1.9803969e-04 1.9801044e-04 1.9803554e-04\n",
            "  1.9811884e-04 1.9824998e-04 1.9840334e-04 1.9852389e-04 1.9859600e-04\n",
            "  1.9860624e-04]]\n",
            "0.9947383\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_val[1])\n",
        "print(np.max(Y_val[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tckiQMTmU-_m",
        "outputId": "4f746805-6172-4f6d-fc41-f61110d6985f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model_eagle.evaluate(np.expand_dims(X_val[1,:,:], axis=0),np.expand_dims(Y_val[1,:], axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q86PEWUpZE93",
        "outputId": "a588cc33-a37f-4dd2-90e7-13781cb10f8d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 79ms/step - loss: 4.0995 - accuracy: 0.7198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[50].T)\n",
        "print(np.max(predictions[50]))\n",
        "# Apply threshold of 0.5 to convert probabilities to binary labels\n",
        "binary_labels = (predictions[50,:] >= 0.5).astype(int)\n",
        "print(binary_labels.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG_2wcq4TNfC",
        "outputId": "8a24542f-bd43-44d2-be67-8f31dc187c21"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.79759991e-02 4.11665998e-02 2.04048753e-02 1.16588390e-02\n",
            "  7.34589295e-03 4.88874502e-03 3.31032672e-03 2.21128645e-03\n",
            "  1.42728782e-03 8.89920106e-04 5.54063066e-04 3.66202876e-04\n",
            "  2.70629302e-04 2.25536947e-04 2.06013297e-04 1.98628011e-04\n",
            "  1.96391702e-04 1.95879664e-04 1.95651941e-04 1.95292057e-04\n",
            "  1.94807930e-04 1.94317239e-04 1.93922446e-04 1.93673375e-04\n",
            "  1.93572196e-04 1.93595260e-04 1.93705884e-04 1.93875501e-04\n",
            "  1.94076740e-04 1.94295950e-04 1.94518580e-04 1.94741267e-04\n",
            "  1.94961060e-04 1.95175322e-04 1.95377157e-04 1.95566710e-04\n",
            "  1.95744680e-04 1.95913104e-04 1.96074194e-04 1.96231093e-04\n",
            "  1.96376175e-04 1.96511406e-04 1.96636232e-04 1.96765453e-04\n",
            "  1.96881054e-04 1.96971538e-04 1.97055881e-04 1.97150061e-04\n",
            "  1.97248024e-04 1.97349043e-04 1.97435802e-04 1.97508474e-04\n",
            "  1.97570247e-04 1.97639209e-04 1.97724046e-04 1.97790796e-04\n",
            "  1.97843212e-04 1.97883594e-04 1.97938905e-04 1.97995148e-04\n",
            "  1.98051232e-04 1.98106558e-04 1.98156617e-04 1.98199326e-04\n",
            "  1.98241090e-04 1.98280992e-04 1.98321621e-04 1.98361158e-04\n",
            "  1.98402398e-04 1.98438909e-04 1.98465787e-04 1.98488124e-04\n",
            "  1.98519134e-04 1.98556067e-04 1.98585578e-04 1.98614376e-04\n",
            "  1.98635578e-04 1.98650174e-04 1.98665148e-04 1.98687281e-04\n",
            "  1.98707756e-04 1.98730297e-04 1.98757596e-04 1.98780530e-04\n",
            "  1.98796450e-04 1.98812748e-04 1.98829817e-04 1.98852169e-04\n",
            "  1.98891052e-04 1.98912880e-04 1.98908703e-04 1.98897687e-04\n",
            "  1.98910595e-04 1.98928799e-04 1.98943992e-04 1.98965034e-04\n",
            "  1.98979062e-04 1.98993293e-04 1.98999172e-04 1.99007918e-04\n",
            "  1.99021204e-04 1.99033515e-04 1.99042828e-04 1.99051574e-04\n",
            "  1.99055925e-04 1.99062939e-04 1.99068629e-04 1.99072063e-04\n",
            "  1.99077753e-04 1.99082686e-04 1.99089511e-04 1.99098053e-04\n",
            "  1.99111935e-04 1.99115340e-04 1.99122922e-04 1.99123679e-04\n",
            "  1.99124450e-04 1.99147617e-04 1.99163565e-04 1.99163944e-04\n",
            "  1.99164526e-04 1.99153888e-04 1.99151415e-04 1.99175352e-04\n",
            "  1.99237256e-04 1.99238013e-04 1.99182919e-04 1.99143455e-04\n",
            "  1.99131857e-04 1.99088186e-04 1.99072427e-04 1.99105460e-04\n",
            "  1.99148169e-04 1.99171336e-04 1.99116868e-04 1.99025744e-04\n",
            "  1.98845170e-04 1.98625174e-04 1.98312759e-04 1.97127127e-04\n",
            "  1.54736510e-04 1.60946529e-05 2.85069927e-06 1.33881088e-06\n",
            "  8.37888308e-07 6.38564302e-07 5.40796862e-07 4.79304106e-07\n",
            "  4.38677603e-07 4.13755799e-07 4.03731605e-07 3.87902929e-07\n",
            "  3.70973936e-07 3.60957557e-07 3.56929547e-07 3.35274251e-07\n",
            "  2.83898743e-07 2.58796632e-07 2.41734199e-07 2.40315046e-07\n",
            "  2.35223695e-07 2.35825638e-07 2.24687170e-07 2.39092259e-07\n",
            "  2.69518921e-07 5.69075837e-07 4.73803220e-06 3.23317690e-05\n",
            "  1.68040686e-04 6.28040754e-04 1.72666938e-03 3.58035509e-03\n",
            "  6.04294334e-03 8.70129745e-03 1.12088835e-02 1.34081747e-02\n",
            "  1.52156260e-02 1.66057013e-02 1.75826065e-02 1.81796905e-02\n",
            "  1.84443053e-02 1.84327234e-02 1.82130318e-02 1.78379007e-02\n",
            "  1.73545182e-02 1.67991333e-02 1.61952209e-02 1.55577511e-02\n",
            "  1.48940170e-02 1.42090619e-02 1.35018006e-02 1.27720237e-02\n",
            "  1.20124659e-02 1.12228133e-02 1.04035996e-02 9.55203921e-03\n",
            "  8.66856985e-03 7.75462296e-03 6.81487983e-03 5.85879199e-03\n",
            "  4.89806477e-03 3.95017490e-03 3.04039801e-03 2.20426871e-03\n",
            "  1.48694031e-03 9.32690629e-04 5.61358873e-04 3.49413283e-04\n",
            "  2.44562980e-04 1.98791531e-04 1.82304007e-04 1.79297364e-04\n",
            "  1.81631840e-04 1.85248166e-04 1.88380625e-04 1.90551727e-04\n",
            "  1.91893472e-04 1.92664942e-04 1.93127533e-04 1.93444328e-04\n",
            "  1.93705491e-04 1.93938366e-04 1.94172986e-04 1.94432898e-04\n",
            "  1.94687978e-04 1.94916080e-04 1.95119312e-04 1.95325192e-04\n",
            "  1.95527929e-04 1.95709232e-04 1.95867891e-04 1.96025387e-04\n",
            "  1.96182664e-04 1.96329347e-04 1.96456138e-04 1.96585592e-04\n",
            "  1.96709152e-04 1.96829627e-04 1.96935493e-04 1.97039364e-04\n",
            "  1.97142916e-04 1.97240122e-04 1.97325324e-04 1.97406436e-04\n",
            "  1.97487010e-04 1.97567628e-04 1.97641682e-04 1.97709902e-04\n",
            "  1.97777234e-04 1.97840607e-04 1.97900386e-04 1.97963236e-04\n",
            "  1.98019494e-04 1.98065187e-04 1.98107111e-04 1.98153037e-04\n",
            "  1.98201014e-04 1.98252994e-04 1.98297625e-04 1.98343754e-04\n",
            "  1.98374008e-04 1.98393303e-04 1.98420748e-04 1.98462367e-04\n",
            "  1.98499474e-04 1.98530310e-04 1.98560796e-04 1.98590526e-04\n",
            "  1.98615890e-04 1.98638998e-04 1.98663256e-04 1.98693742e-04\n",
            "  1.98716662e-04 1.98739595e-04 1.98759502e-04 1.98775218e-04\n",
            "  1.98786802e-04 1.98805166e-04 1.98827358e-04 1.98848022e-04\n",
            "  1.98867157e-04 1.98883645e-04 1.98893540e-04 1.98906622e-04\n",
            "  1.98924448e-04 1.98936570e-04 1.98943584e-04 1.98954789e-04\n",
            "  1.98966169e-04 1.98976981e-04 1.98988942e-04 1.99001268e-04\n",
            "  1.99011876e-04 1.99022150e-04 1.99032016e-04 1.99040936e-04\n",
            "  1.99049478e-04 1.99056289e-04 1.99061804e-04 1.99068076e-04\n",
            "  1.99074348e-04 1.99079645e-04 1.99084388e-04 1.99088754e-04\n",
            "  1.99093120e-04 1.99102418e-04 1.99108690e-04 1.99115340e-04\n",
            "  1.99120841e-04 1.99125410e-04 1.99129950e-04 1.99138129e-04\n",
            "  1.99147413e-04 1.99150090e-04 1.99148752e-04 1.99150847e-04\n",
            "  1.99159753e-04 1.99169459e-04 1.99172311e-04 1.99168702e-04\n",
            "  1.99170972e-04 1.99176691e-04 1.99181231e-04 1.99180067e-04\n",
            "  1.99187489e-04 1.99209142e-04 1.99214075e-04 1.99199087e-04\n",
            "  1.99197937e-04 1.99209724e-04 1.99218455e-04 1.99222442e-04\n",
            "  1.99225498e-04 1.99224916e-04 1.99223578e-04 1.99215807e-04\n",
            "  1.99210859e-04 1.99216927e-04 1.99214264e-04 1.99206290e-04\n",
            "  1.99202128e-04 1.99212562e-04 1.99223010e-04 1.99229660e-04\n",
            "  1.99233647e-04 1.99229660e-04 1.99220915e-04 1.99219401e-04\n",
            "  1.99223767e-04 1.99226051e-04 1.99226241e-04 1.99229296e-04\n",
            "  1.99231945e-04 1.99232891e-04 1.99236325e-04 1.99235743e-04\n",
            "  1.99229660e-04 1.99225673e-04 1.99229296e-04 1.99238595e-04\n",
            "  1.99241636e-04 1.99245042e-04 1.99244285e-04 1.99244285e-04\n",
            "  1.99244096e-04 1.99244663e-04 1.99246759e-04 1.99251896e-04\n",
            "  1.99253787e-04 1.99253234e-04 1.99251692e-04 1.99258546e-04\n",
            "  1.99262897e-04 1.99259492e-04 1.99271642e-04 1.99280956e-04\n",
            "  1.99270697e-04 1.99258546e-04 1.99257032e-04 1.99256829e-04\n",
            "  1.99261194e-04 1.99263479e-04 1.99268034e-04 1.99268412e-04\n",
            "  1.99268616e-04 1.99269925e-04 1.99270886e-04 1.99267452e-04\n",
            "  1.99268237e-04 1.99271642e-04 1.99264614e-04 1.99252841e-04\n",
            "  1.99248461e-04 1.99251503e-04 1.99250557e-04 1.99249218e-04\n",
            "  1.99251692e-04 1.99262344e-04 1.99271090e-04 1.99268994e-04\n",
            "  1.99269547e-04 1.99270507e-04 1.99267655e-04 1.99268616e-04\n",
            "  1.99273738e-04 1.99271642e-04 1.99268412e-04 1.99266331e-04\n",
            "  1.99264803e-04 1.99264803e-04 1.99266709e-04 1.99268790e-04\n",
            "  1.99265196e-04 1.99263275e-04 1.99267844e-04 1.99277347e-04\n",
            "  1.99276576e-04 1.99277536e-04 1.99275062e-04 1.99293689e-04\n",
            "  1.99313072e-04 1.99303176e-04 1.99287606e-04 1.99293136e-04\n",
            "  1.99310220e-04 1.99293136e-04 1.99264236e-04 1.99246948e-04\n",
            "  1.99262722e-04 1.99289701e-04 1.99309288e-04 1.99302420e-04\n",
            "  1.99276779e-04 1.99259666e-04 1.99257396e-04 1.99264439e-04\n",
            "  1.99275266e-04 1.99275441e-04 1.99264046e-04 1.99252478e-04\n",
            "  1.99261194e-04 1.99269547e-04 1.99270129e-04 1.99268412e-04\n",
            "  1.99268994e-04 1.99265196e-04 1.99261194e-04 1.99259492e-04\n",
            "  1.99257207e-04 1.99257207e-04 1.99264439e-04 1.99271090e-04\n",
            "  1.99274684e-04 1.99275266e-04 1.99270333e-04 1.99265778e-04\n",
            "  1.99267655e-04 1.99271453e-04 1.99267844e-04 1.99263479e-04\n",
            "  1.99267277e-04 1.99275266e-04 1.99278860e-04 1.99280956e-04\n",
            "  1.99280956e-04 1.99278496e-04 1.99276779e-04 1.99279631e-04\n",
            "  1.99286078e-04 1.99286660e-04 1.99279253e-04 1.99268790e-04\n",
            "  1.99261762e-04 1.99263101e-04 1.99266535e-04 1.99269751e-04\n",
            "  1.99272792e-04 1.99269925e-04 1.99269751e-04 1.99264992e-04\n",
            "  1.99262344e-04 1.99259885e-04 1.99262344e-04 1.99266535e-04\n",
            "  1.99268412e-04 1.99266709e-04 1.99265196e-04 1.99270129e-04\n",
            "  1.99270507e-04 1.99268616e-04 1.99263857e-04 1.99264439e-04\n",
            "  1.99265560e-04 1.99273738e-04 1.99268994e-04 1.99260801e-04]]\n",
            "0.097976\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_val[50])\n",
        "print(np.max(Y_val[50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVsSJRYDVL0G",
        "outputId": "ca96a4d7-e5ec-469e-9e50-6f3d48a04d95"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[50].T)\n",
        "print(np.max(predictions[50]))\n",
        "\n",
        "print(Y_val[50])\n",
        "print(np.max(Y_val[50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3qcsGRYgw2H",
        "outputId": "e9feff79-cb00-4754-96da-b3193ff462de"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.3152868e-03 9.5999995e-03 9.1443863e-03 8.5982513e-03 8.2309814e-03\n",
            "  8.0632390e-03 8.0288537e-03 8.0559533e-03 8.0954954e-03 8.1244325e-03\n",
            "  8.1381183e-03 8.1406645e-03 8.1379153e-03 8.1341285e-03 8.1313392e-03\n",
            "  8.1299441e-03 8.1295818e-03 8.1297364e-03 8.1300242e-03 8.1302440e-03\n",
            "  8.1303595e-03 8.1303865e-03 8.1303669e-03 8.1303446e-03 8.1303213e-03\n",
            "  8.1303138e-03 8.1303101e-03 8.1303054e-03 8.1303101e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 1.1648042e-02\n",
            "  9.9865061e-01 9.9998915e-01 9.9996597e-01 9.9994457e-01 9.9997628e-01\n",
            "  9.9999505e-01 9.9999106e-01 9.9998379e-01 9.9998921e-01 9.9998873e-01\n",
            "  9.9999642e-01 9.9999094e-01 9.9999213e-01 9.9999851e-01 9.9999827e-01\n",
            "  9.9999791e-01 9.9999863e-01 9.9999666e-01 9.9999976e-01 9.9999392e-01\n",
            "  9.9999934e-01 9.9999666e-01 9.9995381e-01 9.9999446e-01 9.9849892e-01\n",
            "  9.3225318e-01 2.3291765e-01 2.0888481e-05 1.5622709e-07 1.1495354e-09\n",
            "  1.0944388e-09 1.8739307e-08 8.5773860e-07 3.7766062e-05 8.3204103e-04\n",
            "  6.5354123e-03 1.7314468e-02 1.9761633e-02 1.4643261e-02 1.0144231e-02\n",
            "  7.9376185e-03 7.2013047e-03 7.2061950e-03 7.5066900e-03 7.8357682e-03\n",
            "  8.0651278e-03 8.1733949e-03 8.1958370e-03 8.1781186e-03 8.1525119e-03\n",
            "  8.1339097e-03 8.1251543e-03 8.1235413e-03 8.1253266e-03 8.1277844e-03\n",
            "  8.1296135e-03 8.1305401e-03 8.1307897e-03 8.1307022e-03 8.1305169e-03\n",
            "  8.1303744e-03 8.1302943e-03 8.1302747e-03 8.1302822e-03 8.1302980e-03\n",
            "  8.1303101e-03 8.1303138e-03 8.1303176e-03 8.1303176e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03 8.1303138e-03\n",
            "  8.1303138e-03]]\n",
            "0.99999976\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fVLUkiMVWsL",
        "outputId": "f1630e5d-f145-4eb4-fe32-e0cc99de2d50"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7341588"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "loaded_model = keras.models.load_model('/content/model_1.keras')"
      ],
      "metadata": {
        "id": "hudjBx4Dvus0"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = loaded_model.evaluate(X_val, Y_val)\n",
        "print(loss)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcb2zWfz0Q0q",
        "outputId": "5bfaf9f4-2fc2-497c-deeb-76ded3a71ec0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 71ms/step - loss: 0.2799 - accuracy: 0.9796\n",
            "0.27992844581604004\n",
            "0.9796304106712341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rand_model = model()\n",
        "\n",
        "# opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "# rand_model.compile(loss=weighted_binary_crossentropy, optimizer=opt, metrics=[\"accuracy\"])\n",
        "# loss, acc = x.evaluate(X_val, Y_val)\n"
      ],
      "metadata": {
        "id": "_e4-OGpi1gKv"
      },
      "execution_count": 62,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}